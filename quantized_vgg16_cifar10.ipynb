{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantized_vgg16-cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdb0zu7rromz",
        "outputId": "75e12bf0-1caf-48b3-b11d-93cbb4b8f1fc"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y992mMGQOJ8w",
        "outputId": "cc797bab-9e0d-4ba3-f705-0049e8712121"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue4di4KxWTAi"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Convolution2D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "import logging\n",
        "from keras.models import load_model\n",
        "#from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "from keras import regularizers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-JHUtVhK-wu"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMo4JapcK-zU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6430f6c-b7b6-4712-f59c-ad6d88c13c56"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hwDGC6AK-4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea65fb48-bda3-4b49-e3ed-edea6462191d"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a29rZOUv5i2"
      },
      "source": [
        "model = Sequential()\n",
        "weight_decay = 0.0005\n",
        "x_shape = [32,32,3]\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIka4JsiwCHc",
        "outputId": "456254fc-5c8c-44b4-f537-74a68b7a94a4"
      },
      "source": [
        "learning_rate = 0.1\n",
        "lr_decay = 1e-6\n",
        "lr_drop = 20\n",
        "\n",
        "sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uFVpvsTwCRU"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    \n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "# (std, mean, and principal components if ZCA whitening is applied)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfwydtNLwQdN"
      },
      "source": [
        "def lr_scheduler(epoch):\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "    \n",
        "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU91ROB6CIeH"
      },
      "source": [
        "batch_size = 128 \n",
        "maxepoches = 10\n",
        "\n",
        "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "\n",
        "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                            epochs=maxepoches,\n",
        "                            validation_data=(x_test, y_test),callbacks=[reduce_lr], verbose=1)\n",
        "\n",
        "model.save_weights('Quant_cifar10vgg.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNHAFXDCer-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35347ec-893a-49d1-f60d-b3830762126f"
      },
      "source": [
        "model.load_weights('Quant_cifar10vgg.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.738170862197876\n",
            "Test accuracy: 0.6229000091552734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN-F9wF8Jrk4"
      },
      "source": [
        "from keras.layers import Input"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR5mJs4kzWQa"
      },
      "source": [
        "convparams = 320 + 18496\n",
        "trainable_params = model.count_params()\n",
        "footprint = trainable_params * 4\n",
        "\n",
        "#print (\"Memory footprint per Image Feed Forward ~= \" , footprint / 1024.0 /1024.0 ,\"Mb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DBdzmsOh6ap"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def prepare_softtargets(model,X):\n",
        "    inp = model.input                                           # input placeholder\n",
        "    outputs = []\n",
        "    for layer in model.layers[:]:\n",
        "        if layer.name == 'flatten_1':\n",
        "            outputs.append(layer.output)\n",
        "        if layer.name == 'dense_2':\n",
        "            outputs.append(layer.output)\n",
        "            \n",
        "    functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
        "    layer_outs = functor([X, 1.])\n",
        "    return np.array(layer_outs[0]) , np.array(layer_outs[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra3E3sve2-D4",
        "outputId": "30341ad8-7aef-40f5-d7cb-1af19baa6a41"
      },
      "source": [
        "lastconv_out = []\n",
        "logit_out = []\n",
        "for i in range(0,60):\n",
        "    print(\"Batch # : \",i)\n",
        "    l,l2 =  (prepare_softtargets(model,x_train[i*1000:(i+1)*1000]))\n",
        "    lastconv_out.append(l)\n",
        "    logit_out.append(l2)\n",
        "    #lastconv_out , logit_out = prepare_softtargets(model,x_train[i*1000:(i+1)*1000])\n",
        "\n",
        "lastconv_out = np.array(lastconv_out)\n",
        "logit_out = np.array(logit_out)\n",
        "lastconv_out = lastconv_out.reshape((60000 , 9216))\n",
        "logit_out = logit_out.reshape((60000 , 10))\n",
        "\n",
        "x_train = 0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch # :  0\n",
            "Batch # :  1\n",
            "Batch # :  2\n",
            "Batch # :  3\n",
            "Batch # :  4\n",
            "Batch # :  5\n",
            "Batch # :  6\n",
            "Batch # :  7\n",
            "Batch # :  8\n",
            "Batch # :  9\n",
            "Batch # :  10\n",
            "Batch # :  11\n",
            "Batch # :  12\n",
            "Batch # :  13\n",
            "Batch # :  14\n",
            "Batch # :  15\n",
            "Batch # :  16\n",
            "Batch # :  17\n",
            "Batch # :  18\n",
            "Batch # :  19\n",
            "Batch # :  20\n",
            "Batch # :  21\n",
            "Batch # :  22\n",
            "Batch # :  23\n",
            "Batch # :  24\n",
            "Batch # :  25\n",
            "Batch # :  26\n",
            "Batch # :  27\n",
            "Batch # :  28\n",
            "Batch # :  29\n",
            "Batch # :  30\n",
            "Batch # :  31\n",
            "Batch # :  32\n",
            "Batch # :  33\n",
            "Batch # :  34\n",
            "Batch # :  35\n",
            "Batch # :  36\n",
            "Batch # :  37\n",
            "Batch # :  38\n",
            "Batch # :  39\n",
            "Batch # :  40\n",
            "Batch # :  41\n",
            "Batch # :  42\n",
            "Batch # :  43\n",
            "Batch # :  44\n",
            "Batch # :  45\n",
            "Batch # :  46\n",
            "Batch # :  47\n",
            "Batch # :  48\n",
            "Batch # :  49\n",
            "Batch # :  50\n",
            "Batch # :  51\n",
            "Batch # :  52\n",
            "Batch # :  53\n",
            "Batch # :  54\n",
            "Batch # :  55\n",
            "Batch # :  56\n",
            "Batch # :  57\n",
            "Batch # :  58\n",
            "Batch # :  59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3B9-obI3q4D"
      },
      "source": [
        "h5f = h5py.File('lastconv_out.h5', 'w')\n",
        "h5f.create_dataset('dataset_1', data=lastconv_out)\n",
        "h5f.close()\n",
        "\n",
        "h5f2 = h5py.File('logit_out.h5', 'w')\n",
        "h5f2.create_dataset('dataset_1', data=logit_out)\n",
        "h5f2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIXfqhyt3qtm",
        "outputId": "11516eaa-9d70-4f0c-b225-a31fff40fb41"
      },
      "source": [
        "lastconv_out = 0\n",
        "logit_out = 0 \n",
        "\n",
        "test_lastconv_out = []\n",
        "test_logit_out = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    print(\"Batch # : \",i)\n",
        "    l,l2 =  prepare_softtargets(model,x_test[i*1000:(i+1)*1000])\n",
        "    test_lastconv_out.append(l)\n",
        "    test_logit_out.append(l2)\n",
        "    \n",
        "\n",
        "# lastconv_out.shape , logit_out.shape\n",
        "test_lastconv_out = np.array(test_lastconv_out)\n",
        "test_logit_out = np.array(test_logit_out)\n",
        "\n",
        "test_lastconv_out = test_lastconv_out.reshape((10000 , 9216))\n",
        "test_logit_out = test_logit_out.reshape((10000 , 10))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch # :  0\n",
            "Batch # :  1\n",
            "Batch # :  2\n",
            "Batch # :  3\n",
            "Batch # :  4\n",
            "Batch # :  5\n",
            "Batch # :  6\n",
            "Batch # :  7\n",
            "Batch # :  8\n",
            "Batch # :  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKJzOVd4SRP"
      },
      "source": [
        "h5f = h5py.File('test_lastconv_out.h5', 'w')\n",
        "h5f.create_dataset('dataset_1', data=test_lastconv_out)\n",
        "h5f.close()\n",
        "\n",
        "h5f2 = h5py.File('test_logit_out.h5', 'w')\n",
        "h5f2.create_dataset('dataset_1', data=test_logit_out)\n",
        "h5f2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NemGhHT8FVx"
      },
      "source": [
        "def softmax_c(z):\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis]\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis] \n",
        "    return e_x / div"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgtoMv0q5SRD"
      },
      "source": [
        "## Logit Regression Method \n",
        "\n",
        "results = []\n",
        "\n",
        "for HiddenNeuron in [2,3,4,5,6,7]:\n",
        "    \n",
        "    # Load input,target to studentModel \n",
        "    h5f = h5py.File('lastconv_out.h5', 'r')\n",
        "    lastconv_out = h5f['dataset_1'][:]\n",
        "    h5f.close()\n",
        "    \n",
        "    h5f2 = h5py.File('logit_out.h5', 'r')\n",
        "    logit_out = h5f2['dataset_1'][:]\n",
        "    h5f2.close()\n",
        "\n",
        "    s_model = Sequential()\n",
        "    s_model.add(Dense(HiddenNeuron,input_dim=9216,activation='relu'))\n",
        "    s_model.add(Dropout(0.2))\n",
        "    s_model.add(Dense(num_classes))\n",
        "\n",
        "    s_model.compile(loss='mse',\n",
        "                  optimizer=keras.optimizers.Adadelta(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    s_model.fit(lastconv_out, logit_out, epochs=40, verbose=0)\n",
        "#     s_model.save_weights(\"student_weights_\"+str(HiddenNeuron)+\"hidden_0.5_dropout.h5\")\n",
        "    \n",
        "    compressionRate = model.count_params() / np.float(s_model.count_params()  + convparams)\n",
        "    \n",
        "    lastconv_out = 0\n",
        "    logit_out = 0                \n",
        "    \n",
        "    h5f = h5py.File('test_lastconv_out.h5', 'r')\n",
        "    test_lastconv_out = h5f['dataset_1'][:]\n",
        "    h5f.close()\n",
        "    h5f2 = h5py.File('test_logit_out.h5', 'r')\n",
        "    test_logit_out = h5f2['dataset_1'][:]\n",
        "    h5f2.close()\n",
        "    \n",
        "    pred = s_model.predict(test_lastconv_out)\n",
        "    probs = softmax_c(pred)\n",
        "    pred_classes = np.argmax(probs,axis=1)\n",
        "\n",
        "    accuracy_student = metrics.accuracy_score(y_pred=pred_classes,y_true=np.argmax(y_test,axis=1))\n",
        "                            \n",
        "    s_model = 0 \n",
        "    lastconv_out = 0\n",
        "    logit_out = 0 \n",
        "    test_lastconv_out = 0\n",
        "    test_logit_out = 0 \n",
        "    \n",
        "    #results.append(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QGbqqfH6dIC"
      },
      "source": [
        "#s_model.save_weights('vgg16_cifar10.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}